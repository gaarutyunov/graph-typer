\subsection{Graph Representation of Code}\label{subsec:graph-representation-of-code}

AST and DFG have already been used with Transformers in the code generation and summarization tasks~\cite{wang_unified_2022,tang_ast-transformer_2021,sun_treegen_2020}.
In addition, some joint graph structure representations that include different code graphs have been developed,
namely code property graph (CPG)~\cite{yamaguchi2014modeling}, that incorporates AST, CFG and PDG (program dependency graph).
This graph representation has already been used for vulnerability detection~\cite{yamaguchi2014modeling} and similarity detection~\cite{liu2023learning}.

\subsection{Graph Transformers}\label{subsec:graph-transformers}

Graph Transformers is a novel architecture that has been developing in the past few years.
They have been applied for several tasks, mostly in the field of molecule generation, node classification and node feature regression~\cite{kim_pure_2022,kreuzer_rethinking_2021,dwivedi_generalization_2021,ying_transformers_2021}.
Apart from models with alterations to Transformer base architecture~\cite{ying_transformers_2021, kreuzer_rethinking_2021}
researchers have recently developed simpler models~\cite{kim_pure_2022} that are compatible with many popular techniques developed for
standard Transformers~\cite{choromanski_rethinking_2020}.

\subsection{Type Inference with Neural Networks}\label{subsec:type-inference-with-neural-networks}

The task of type inference has been also extensively covered in recent research.
Many different architectures have been used for this task: GNNs~\cite{allamanis2020typilus}, RNNs~\cite{pradel2020typewriter, mir_type4py_2021} and Transformers~\cite{jesse2021typebert, peng2023generative} among others.
Moreover, graph representation of code has been used for the task of type inference in dynamically typed programming languages such as Python~\cite{allamanis2020typilus} and Javascript~\cite{schrouff_inferring_2019}.

However, the power of Graph Transformers and Graph Representation of code hasn't been combined yet to solve the task of
type inference in source code.
This is the gap our model aims to fill.
The results of our model compared to previous work~\cite{mir_type4py_2021, allamanis2020typilus, pradel2020typewriter, jesse2021typebert, peng2023generative} are displayed in Table~\ref{tab:results}.

\begin{table}
    \centering
    \caption{Quantitative evaluation of models measuring their ability to
    predict ground truth type annotations.}
    \label{tab:results}
    \input{tables/result}
\end{table}