\subsection{Dataset}\label{subsec:dataset}

To train and test the model we gathered 600 Python repositories from GitHub containing type annotations from Typilus~\cite{allamanis2020typilus}.
We clone these repositories and use pytype~\cite{PyType} for static analysis, augmenting the corpus with inferred type annotations.
The top 175 most downloaded libraries are added to the Python environment for type inference.
Through deduplication, we remove over 133 thousand code duplicates to prevent bias.

The resulting dataset comprises 118,440 files with 5,997,459 symbols, of which 252,470 have non-Any non-None type annotations.
The annotations exhibit diversity with a heavy-tailed distribution, where the top 10 types cover half of the dataset, primarily including str, bool, and int.
Only 158 types have over 100 annotations, while the majority of types are used fewer than 100 times each, forming 32\% of the dataset.
This distribution underscores the importance of accurately predicting annotations, especially for less common types.
The long-tail of types consists of user-defined and generic types with various type arguments.

In addition to extracting graphs from source code AST, we split them by setting a maximum node and edges number in one graph.
For this, we prune the graphs around nodes that have annotations that are later used as targets during training and testing.
Finally, we split the data into train-validation-test sets with proportions of 70-10-20, respectively.

\subsection{Model Architecture}\label{subsec:model-architecture}

We base our model architecture on TokenGT~\cite{kim_pure_2022}.
The main advantage of this model is that standard Transformer architecture is not altered to support graph data.
It allows us to use some advantages developed specifically for Transformers.
For instance, Performer~\cite{choromanski_rethinking_2020} is used to speed up training by using linear time as space complexity.

The main idea of the authors is that combining appropriate token-wise embeddings and self-attention over the node and edge tokens
is expressive enough to accurately encode graph structure to make graph and node-wise predictions.
The embeddings in the model are composed of orthonormal node identifiers, namely Laplacian eigenvectors obtained from
eigendecomposition of graph Laplacian matrix.
In addition, type identifiers are used to encode types of tokens (nodes or edges).

\begin{figure*}
    \resizebox{\textwidth}{!}{\input{figures/model.tikz}}
    \caption{GraphTyper Architecture. The source code is first transformed into AST graph, then type annotations are randomely masked. The graph is enriched by token type identifiers (node or edge) and orthonormal node identifiers obtained from eigendecomposition of Laplacian matrix. The resulting graph is fed through a Transformer Encoder to obtain type annotations for masked nodes.}
    \label{fig:model}
\end{figure*}

In our model, we use node and edge types extracted from code as token features.
Node ground truth annotations are added to the features and randomly masked during training.
The overall architecture of the model is displayed at Figure~\ref{fig:model}.

\subsubsection{Masked Transformer Encoder Model}

Predicting type annotations in graph domain is a node classification task.
However, since we are using a Pure Transformer with graphs represented as a sequence of tokens, the task can be reduced to token classification.
In the Natural Language Processing (NLP) domain, this is a ubiquitous task, also known as Named Entity Recognition (NER).

Encoder-only architecture has been widely used for the NER task, namely BERT is one of the most popular models~\cite{liu2021nerbert,Darji_2023}.
We adapt similar architecture by randomly masking type annotations.
We then apply an MLP layer to the output of TokenGT~\cite{kim_pure_2022} to get logits of type annotations.

Masked model architecture is very versatile, and the pretrained model can be later easily fine-tuned for other tasks,
similar to the approaches from the NLP-domain~\cite{liu2021nerbert}.
For example, error~\cite{bieber2022static} and vulnerability~\cite{sun2023exploring} data can be added to the code graph to detect and fix them~\cite{nguyen_regvd_2021,li_vuldeepecker_2018,cao_bgnn4vd_2021,li_sysevr_2021,russell_automated_2018}.