\documentclass[conference]{IEEEtran}
\usepackage[
    style=numeric,
    sorting=none
]{biblatex}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs, multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning,fit,backgrounds,scopes,decorations.pathreplacing,shapes.geometric}
\newcommand{\bibliofont}{\footnotesize}
\renewcommand\IEEEkeywordsname{Keywords}
\renewcommand\arraystretch{1.5}

\addbibresource{paper.bib}

\begin{document}

\author{
    \IEEEauthorblockN{{\bf German Arutyunov}\IEEEauthorrefmark{1}\\ \tt\footnotesize gaarutyunov@edu.hse.ru} \and
    \IEEEauthorblockN{{\bf Sergey Avdoshin}\IEEEauthorrefmark{1}\\ \tt\footnotesize savdoshin@hse.ru}
    \and
    \IEEEauthorblockA{\IEEEauthorrefmark{1}HSE University, 20, Myasnitskaya st., Moscow, Russia}
}

\title{GraphTyper: Neural Types Inference from Code Represented as Graph}

\begin{abstract}
    Although software development is mostly a creative process, there are many scrutiny tasks.
    As in other industries, there is a trend for automation of routine work.
    In many cases, machine learning and neural networks have become a useful assistant in that matter.
    Programming is not an exception:
    GitHub has stated that Copilot is already used to write up to 30\% of code in the company.
    Copilot is based on Codex, a Transformer model trained on code as a sequence.
    However, a sequence is not a perfect representation for programming languages.
    In this work, we claim and demonstrate that by combining the advantages of Transformers
    and graph representations of code, it is possible to achieve excellent results even with comparably small models.
\end{abstract}

\begin{IEEEkeywords}
    Neural networks, Transformers, graphs, abstract syntax tree
\end{IEEEkeywords}

\maketitle

\section{Introduction}\label{sec:introduction}
\input{sections/introduction}

\section{Problem Statement}\label{sec:problem-statement}
\input{sections/problem-statement}

\section{Previous Work}\label{sec:previous-work}
\input{sections/previous-work}

\section{Proposed Solution}\label{sec:proposed-solution}
\input{sections/proposed-solution}

\section{Experiments and Ablation Results}\label{sec:experiment-results-and-ablation}
\input{sections/experiments}

\section{Known Limitations}\label{sec:known-limitations}
\input{sections/limitations-and-workarounds}

\section{Future Work}\label{sec:future-work}
\input{sections/future-work}

\section{Conclusion}\label{sec:conclusion}

As for the conclusion, we were able to create a universal model based on TokenGT~\cite{kim_pure_2022} and code represented as graphs.
One of the most important advantages of this model is that it uses the code graph directly.
Secondly, the model can be modified to fit other tasks, such as code generation and summarization, docstring generation, refactoring and many more.
The code graph can also be extended by different features and node types, since the representation does not differ depending on graph structure.

\section{Acknowledgments}\label{sec:acknowledgments}

This research was supported in part through computational resources of HPC facilities at HSE University~\cite{kostenetskiy_hpc_2021}.

\printbibliography

\end{document}