To select the final model architecture we test different models by:

\begin{enumerate}
    \item Increasing the number of parameters (Big)
    \item Testing different context length, i.e.\ maximum number of nodes in graph (512 vs 1024)
    \item Validated the necessity of node and type identifiers that encode graph structure (Ablated)
    \item Testing different Transformer architectures (Encoder-only vs Encoder-Decoder)
\end{enumerate}

For our experiments and ablation analysis we train and test the models on one sample repository.
To test the models we calculate Top-n predictions similar to the previous work~\cite{mir_type4py_2021}.
Each model is trained on dataset preprocessed to contain maximum 512 and 1024 nodes in a graph.
Table~\ref{tab:ablation} depicts the results of the experiments and ablation.
As we can see, both models that increase the number of parameters, Big and Deep, also increase the predictive power of the model.

As for the context length, our findings are aligned with the conclusions from previous work~\cite{arutyunov_big_2022}:
longer context increases the performance of the model.
However, the AST representation of source code is very bloated and even having a lot of nodes in the graph might not capture
enough useful information to make quality predictions.
Thus, in future research we will be working on finding a better and more compact graph representation of code.

Finally, our ablation analysis demonstrates that indeed the graph structure embeddings play key role in model quality.
By removing them from the model we are left with a simple Transformer that makes predictions only based on AST nodes and edges types.

Therefore, in the resulting model we increase the number of layers, attention heads, as well as the dimension of the hidden layers.
In addition, we preprocess the complete dataset to have 1024 nodes in graph at most.
Finally, we add the node and type identifiers from the TokenGT paper~\cite{kim_pure_2022}.
The resulting model has 441 million parameters: 12 encoder layers, 16 attention heads, 2048 hidden dimension and 4096 feed-forward layer dimension.

\begin{table}
    \centering
    \caption{Expirement results of Top-n predictions for different model variants.}
    \label{tab:ablation}
    \input{ablation}
\end{table}